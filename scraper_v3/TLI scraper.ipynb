{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c17e752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time, sleep\n",
    "from datetime import date, datetime\n",
    "from dateutil.parser import parse\n",
    "from pyquery import PyQuery\n",
    "import pytz\n",
    "import json  \n",
    "from bson import json_util\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "from numpy.random import randint\n",
    "from uuid import uuid4\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from pymongo.collection import Collection\n",
    "\n",
    "from selenium import webdriver\n",
    "from lxml.html import fromstring\n",
    "import requests\n",
    "import boto3\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "MONGOURL = os.getenv('MONGOURL')\n",
    "DB_NAME = os.getenv('DB_NAME')\n",
    "COLL_NAME = os.getenv('COLL_NAME')\n",
    "BUCKET = os.environ[\"BUCKET\"]\n",
    "REGION_NAME = os.environ[\"REGION_NAME\"]\n",
    "\n",
    "CRAWL_PAGE_COUNT = 2\n",
    "DEBUG = 0\n",
    "\n",
    "crawl_url = \"https://thelogicalindian.com/fact-check\"\n",
    "domain=\"thelogicalindian.com\"\n",
    "lang = \"english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff44ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collection(MONGOURL, DB_NAME, COLL_NAME):\n",
    "    cli = MongoClient(MONGOURL)\n",
    "    db = cli[DB_NAME]\n",
    "    collection = db[COLL_NAME]\n",
    "\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a682cf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aws_connection():\n",
    "\n",
    "    access_id = os.environ[\"ACCESS_ID\"]\n",
    "    access_key = os.environ[\"ACCESS_KEY\"]\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        region_name=\"ap-south-1\",\n",
    "        aws_access_key_id=access_id,\n",
    "        aws_secret_access_key=access_key,\n",
    "    )\n",
    "\n",
    "    return s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa4ca099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree(url):\n",
    "\n",
    "    html = None                        \n",
    "  \n",
    "    try:\n",
    "        html = requests.get(url)\n",
    "    except Exception as e:\n",
    "        print(f\"failed request: {e}\")\n",
    "\n",
    "    html.encoding = \"utf-8\"\n",
    "    tree = fromstring(html.content)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39c64752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_unicode(mangled):\n",
    "    return mangled.encode('latin1','ignore').decode('utf8', 'replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f380d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "pages:   0%|                                                                                     | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered crawler\n",
      "https://thelogicalindian.com/fact-check/fact-check/indira-gandhi-32126 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/assam-rifles-convoy-attacked-32100 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/andhra-pradesh-flood-32096 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/bundelkhand-dam-32062 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/amethi-land-dispute-32061 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/samajwadi-party-song-32049 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/vir-das-32047 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/pm-modi-bow-nehru-32021 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/c-60-commandos-welcome-32020 exists in collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/indo-china-road-31998 exists in collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/namaz-on-roads-31994 exists in collection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "pages:  50%|██████████████████████████████████████▌                                      | 1/2 [00:06<00:06,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://thelogicalindian.com/fact-check/fact-check/amravati-protest-31970 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/nita-ambani-bottle-31969 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/wb-police-cleaned-the-mosque-31946 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/rice-jihad-31943 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/impotence-pills-31923 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/muslims-robbed-couple-31920 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/rashid-alvi-31901 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/australian-chanting-slogans-31900 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/nouf-al-marwaai-padma-shri-31883 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/electric-scooter-explode-31882 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/nisha-dahiya-31859 new to collection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pages: 100%|█████████████████████████████████████████████████████████████████████████████| 2/2 [00:15<00:00,  7.58s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://thelogicalindian.com/fact-check/fact-check/andhra-pradesh-flood-32096',\n",
       " 'https://thelogicalindian.com/fact-check/fact-check/nouf-al-marwaai-padma-shri-31883',\n",
       " 'https://thelogicalindian.com/fact-check/fact-check/samajwadi-party-song-32049',\n",
       " 'https://thelogicalindian.com/fact-check/fact-check/vir-das-32047',\n",
       " 'https://thelogicalindian.com/fact-check/fact-check/amravati-protest-31970',\n",
       " 'https://thelogicalindian.com/fact-check/fact-check/pm-modi-bow-nehru-32021',\n",
       " 'https://thelogicalindian.com/fact-check/fact-check/rashid-alvi-31901',\n",
       " 'https://thelogicalindian.com/fact-check/fact-check/amethi-land-dispute-32061',\n",
       " 'https://thelogicalindian.com/fact-check/fact-check/electric-scooter-explode-31882',\n",
       " 'https://thelogicalindian.com/fact-check/fact-check/indira-gandhi-32126',\n",
       " 'https://thelogicalindian.com/fact-check/fact-check/wb-police-cleaned-the-mosque-31946',\n",
       " 'https://thelogicalindian.com/fact-check/fact-check/muslims-robbed-couple-31920',\n",
       " 'https://thelogicalindian.com/fact-check/fact-check/nisha-dahiya-31859',\n",
       " 'https://thelogicalindian.com/fact-check/fact-check/impotence-pills-31923',\n",
       " 'https://thelogicalindian.com/fact-check/fact-check/australian-chanting-slogans-31900',\n",
       " 'https://thelogicalindian.com/fact-check/fact-check/rice-jihad-31943',\n",
       " 'https://thelogicalindian.com/fact-check/fact-check/bundelkhand-dam-32062',\n",
       " 'https://thelogicalindian.com/fact-check/fact-check/assam-rifles-convoy-attacked-32100',\n",
       " 'https://thelogicalindian.com/fact-check/fact-check/nita-ambani-bottle-31969']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def crawler(crawl_url): \n",
    "\n",
    "    print(\"entered crawler\")\n",
    "\n",
    "    file_name = 'url_list.json'\n",
    "    if os.path.exists(file_name):\n",
    "        print(\"site already been crawled. See \", file_name)\n",
    "        with open(file_name, \"r\") as f:\n",
    "            url_list = json.load(f)\n",
    "    \n",
    "    else:\n",
    "        url_list = []\n",
    "\n",
    "        coll = get_collection(MONGOURL, DB_NAME, COLL_NAME)\n",
    "\n",
    "        for page in tqdm(range(CRAWL_PAGE_COUNT), desc=\"pages: \"):\n",
    "            page_url = f\"{crawl_url}/{page+1}\"\n",
    "            tree = get_tree(page_url)\n",
    "            \n",
    "            if (tree == None):\n",
    "                print(\"No HTML on Link\")\n",
    "                continue\n",
    "\n",
    "            permalinks = PyQuery(tree).find(\".single-article>a\")\n",
    "            \n",
    "            for pl in permalinks:\n",
    "                link = crawl_url + pl.attrib['href']\n",
    "                if 'javascript:void(0)' in link:   #these links should not be scraped\n",
    "                    continue\n",
    "                if coll.count_documents({\"postURL\": link}, {}):\n",
    "                    print(link, \"exists in collection\")\n",
    "                    continue\n",
    "                else:\n",
    "                    print(link, \"new to collection\")\n",
    "                    url_list.append(link)\n",
    "            sleep(randint(5,10))            \n",
    "    \n",
    "        url_list = list(set(url_list))\n",
    "        with open(file_name, 'w') as f:\n",
    "            json.dump(url_list,f)  \n",
    "            \n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7233c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_downloader(url, sub_folder): \n",
    "    print(\"entered downloader\")\n",
    "    print(url)\n",
    "    \n",
    "    #save_path = 'article_one_subfolder'\n",
    "    file = \"file.html\"\n",
    "    file_name = os.path.join(sub_folder, file)\n",
    "    print(file_name)  \n",
    "\n",
    "    if os.path.exists(file_name):\n",
    "        print(\"Article Already Downloaded. Loading from local.\")\n",
    "        with open(file_name, 'rb') as f:\n",
    "            html_text = f.read()\n",
    "    else:\n",
    "        response = requests.get(url)\n",
    "        html_text = response.content\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            f.write(html_text)\n",
    "    \n",
    "    return html_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b6f43f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_info(pq):\n",
    "\n",
    "    headline = pq(\"h1.article-heading\").text()\n",
    "    print(headline)\n",
    "    date = pq('h3.date-info>span').text()\n",
    "    date=date.split(\",\")[1]\n",
    "    date=date.split(\" \")[1:4]\n",
    "    #print(date)\n",
    "    datestr = ' '.join(map(str, date))\n",
    "    print(datestr)\n",
    "    datestr = parse(datestr).astimezone(pytz.timezone('Asia/Calcutta')).strftime(\"%B %d, %Y\")\n",
    "    author_name = pq('h3>a').text().split(':')[1].strip()\n",
    "    author_name = author_name.rsplit(' ', 1)[0]\n",
    "    print(author_name)\n",
    "    author_link = crawl_url + pq('h3>a').attr['href']\n",
    "    print(author_link)\n",
    "    article_info = {\n",
    "        \"headline\": restore_unicode(headline),\n",
    "        \"author\": restore_unicode(author_name),\n",
    "        \"author_link\": restore_unicode(author_link),\n",
    "        \"date_updated\": restore_unicode(datestr),\n",
    "    }\n",
    "    return article_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa87c5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_content(pq):\n",
    "    \n",
    "    content = {\n",
    "        \"text\": [],\n",
    "        \"fb_video\": [],\n",
    "        \"image\": [],\n",
    "        \"tweet\": [],\n",
    "    }\n",
    "\n",
    "    # text content\n",
    "    content['text'] = pq('div.details-content-story').text()\n",
    "\n",
    "    # images\n",
    "    images = pq.find('.article-head-image>.img-wth-credits>img')\n",
    "    images += pq.find('.image-and-caption-wrapper>img')\n",
    "    images = list(dict.fromkeys(images))\n",
    "\n",
    "    for i in images:\n",
    "        if 'src' in i.attrib:\n",
    "            #print(i.attrib[\"src\"])\n",
    "            content[\"image\"].append(i.attrib[\"src\"])      \n",
    "            \n",
    "    #fb_vid = pq.find('.h-embed-wrapper>h-iframe') \n",
    "    #for f in fb_vid:\n",
    "    #    content[\"fb_video\"].append(f.attrib[\"src\"])  \n",
    "    \n",
    "    #twitter videos\n",
    "        \n",
    "    tweets = pq.find('.twitter-tweet>a') \n",
    "    for t in tweets:\n",
    "        content[\"tweet\"].append(t.attrib[\"href\"])  \n",
    "        \n",
    "        \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a755e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_timestamp(item_date_object):\n",
    "    if isinstance(item_date_object, (date, datetime)):\n",
    "        return item_date_object.timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56b738e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_parser(html_text, url, domain, lang, sub_folder):\n",
    "    \n",
    "    print(\"entered article_parser\")\n",
    "    file = \"post.json\"\n",
    "    file_name = os.path.join(sub_folder, file)\n",
    "    if os.path.exists(file_name):\n",
    "        print(\"story has already been parsed.See \", file_name)\n",
    "        with open(file_name, \"r\") as f:\n",
    "            post = json.load(f)\n",
    "    else:\n",
    "        print(\"entered parser\")\n",
    "        pq = PyQuery(html_text)\n",
    "        \n",
    "        # generate post_id\n",
    "        post_id = uuid4().hex\n",
    "\n",
    "        article_info = get_article_info(pq)\n",
    "\n",
    "        # uniform date format\n",
    "        now_date = date.today().strftime(\"%B %d, %Y\")\n",
    "        now_date_utc = datetime.utcnow()\n",
    "        date_updated = article_info[\"date_updated\"]\n",
    "        date_updated_utc = datetime.strptime(date_updated, \"%B %d, %Y\")\n",
    "\n",
    "        author = {\"name\": article_info[\"author\"], \"link\": article_info[\"author_link\"]}  \n",
    "\n",
    "        article_content = get_article_content(pq)\n",
    "        docs = []\n",
    "        for k, v in article_content.items():\n",
    "            if not v:  # empty list\n",
    "                continue\n",
    "            \n",
    "                        \n",
    "            if k == \"text\":  \n",
    "                doc_id = uuid4().hex\n",
    "                doc = {\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"postID\": post_id,\n",
    "                    \"domain\": domain,\n",
    "                    \"origURL\": url, # for text content, URL is the URL of the story\n",
    "                    \"s3URL\": None,\n",
    "                    \"possibleLangs\": lang,\n",
    "                    \"mediaType\": k,\n",
    "                    \"content\": v,  # text, if media_type = text or text in image/audio/video\n",
    "                    \"nowDate\": now_date,  # date of scraping, same as date_accessed\n",
    "                    \"nowDate_UTC\": now_date_utc,\n",
    "                    \"isGoodPrior\": None,  # no of [-ve votes, +ve votes] TODO: Discuss  Discussed: Look More\n",
    "                }\n",
    "                docs.append(doc)\n",
    "            \n",
    "            else:\n",
    "                for media_url in v:\n",
    "                    doc_id = uuid4().hex\n",
    "                    print(doc_id)\n",
    "                    doc = {\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"postID\": post_id,\n",
    "                        \"domain\": domain,\n",
    "                        \"origURL\": media_url,  # for images,videos URL is the URL of the media item.\n",
    "                        \"s3URL\": None,\n",
    "                        \"possibleLangs\": lang,\n",
    "                        \"mediaType\": k,\n",
    "                        \"content\": None,  # this field is specifically to store text content.\n",
    "                        \"nowDate\": now_date,  # date of scraping, same as date_accessed\n",
    "                        \"nowDate_UTC\": now_date_utc,\n",
    "                        \"isGoodPrior\": None,  # no of [-ve votes, +ve votes] TODO: Discuss\n",
    "                    }  \n",
    "                    docs.append(doc)          \n",
    "\n",
    "        post = {\n",
    "            \"postID\": post_id,  # unique post ID\n",
    "            \"postURL\": url,  \n",
    "            \"domain\": domain,  # domain such as altnews/factly\n",
    "            \"headline\": article_info[\"headline\"],  # headline text\n",
    "            \"date_accessed\": now_date,  # date scraped\n",
    "            \"date_accessed_UTC\": now_date_utc,\n",
    "            \"date_updated\": date_updated,  # later of date published/updated\n",
    "            \"date_updated_UTC\": date_updated_utc,  # later of date published/updated\n",
    "            \"author\": author,\n",
    "            \"post_category\": None,\n",
    "            \"claims_review\": None,\n",
    "            \"docs\": docs,\n",
    "        }\n",
    "\n",
    "        print(post)\n",
    "\n",
    "\n",
    "        json_data = json.dumps(post, default=convert_timestamp)\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write(json_data)\n",
    "\n",
    "        \n",
    "    return post "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d88c9899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_images(post,sub_folder):\n",
    "\n",
    "    url = None\n",
    "    filename_dict = {}\n",
    "\n",
    "    for doc in post[\"docs\"]:\n",
    "        if (doc[\"mediaType\"] == 'image'):\n",
    "            url = doc[\"origURL\"]\n",
    "            if url is None:\n",
    "                print(\"Media url is None. Setting s3URL as error...\")\n",
    "                doc[\"s3_url\"] = \"ERR\"\n",
    "            else:\n",
    "                filename = url.split(\"/\")[-1]\n",
    "                \n",
    "                r = requests.get(url)\n",
    "                image = Image.open(BytesIO(r.content)) \n",
    "                if len(filename.split(\".\")) == 1:\n",
    "                        filename = f\"{filename}.{image.format.lower()}\"\n",
    "                if image.mode in (\"RGBA\", \"P\"): \n",
    "                    image = image.convert(\"RGB\")\n",
    "                imgfile=f'{sub_folder}/{filename}'\n",
    "                image.save(f'{sub_folder}/{filename}')\n",
    "                filename_dict.update({doc[\"doc_id\"]: filename})\n",
    "                \n",
    "    return filename_dict\n",
    "\n",
    "\n",
    "def media_downloader(post, sub_folder):\n",
    "    print(\"entered media downloader\")\n",
    "    file_name = f'{sub_folder}/media_dict.json'\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        print(\"media dictionary exists. Some media items may have been dowloaded. See \",file_name)\n",
    "        with open(file_name, \"r\") as f:\n",
    "            media_dict = json.load(f)\n",
    "        \n",
    "    else:    \n",
    "        media_dict = get_all_images(post,sub_folder)\n",
    "        json_data = json.dumps(media_dict, default=convert_timestamp)\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write(json_data)\n",
    "\n",
    "    return media_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3725fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_uploader(post, media_dict, html_text, sub_folder):\n",
    "\n",
    "    print(\"entered data uploader\")\n",
    "\n",
    "    coll = get_collection(MONGOURL, DB_NAME, COLL_NAME)\n",
    "     \n",
    "    if coll.count_documents({\"postURL\": post[\"postURL\"]}, {}):\n",
    "        print(post[\"postURL\"], \"already added to database. If you want to updatex media items, use separate script\")\n",
    "        return 1\n",
    "                \n",
    "\n",
    "    s3 = aws_connection()\n",
    "\n",
    "    for doc in post[\"docs\"]:\n",
    "            if (doc[\"s3URL\"] != None):\n",
    "                print(\"Skipping upload. Doc has an existing s3_url:\",doc[\"s3URL\"])\n",
    "                continue\n",
    "            filename = media_dict.get(doc[\"doc_id\"])\n",
    "            if (filename != None):\n",
    "                s3_filename = str(uuid4())\n",
    "                res = s3.upload_file(\n",
    "                            f'{sub_folder}/{filename}',\n",
    "                            BUCKET,\n",
    "                            s3_filename,\n",
    "                            ExtraArgs={\"ContentType\": \"unk_content_type\"},\n",
    "                        )\n",
    "                s3_url = f\"https://{BUCKET}.s3.{REGION_NAME}.amazonaws.com/{s3_filename}\" \n",
    "                doc[\"s3URL\"] = s3_url\n",
    "                \n",
    "            \n",
    "\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "  \n",
    "    coll.insert_one(post)\n",
    "\n",
    "    s3_html_name = post[\"postURL\"]\n",
    "    file = \"file.html\"\n",
    "    res = s3.upload_file( os.path.join(sub_folder, file),\n",
    "                          BUCKET,\n",
    "                          s3_html_name,\n",
    "                          ExtraArgs={\"ContentType\": \"unk_content_type\"},\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80d80def",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "pages:   0%|                                                                                     | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TLI scraper initiated\n",
      "entered crawler\n",
      "https://thelogicalindian.com/fact-check/fact-check/indira-gandhi-32126 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/assam-rifles-convoy-attacked-32100 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/andhra-pradesh-flood-32096 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/bundelkhand-dam-32062 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/amethi-land-dispute-32061 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/samajwadi-party-song-32049 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/vir-das-32047 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/pm-modi-bow-nehru-32021 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/c-60-commandos-welcome-32020 exists in collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/indo-china-road-31998 exists in collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/namaz-on-roads-31994 exists in collection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "pages:  50%|██████████████████████████████████████▌                                      | 1/2 [00:06<00:06,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://thelogicalindian.com/fact-check/fact-check/amravati-protest-31970 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/nita-ambani-bottle-31969 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/wb-police-cleaned-the-mosque-31946 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/rice-jihad-31943 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/impotence-pills-31923 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/muslims-robbed-couple-31920 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/rashid-alvi-31901 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/australian-chanting-slogans-31900 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/nouf-al-marwaai-padma-shri-31883 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/electric-scooter-explode-31882 new to collection\n",
      "https://thelogicalindian.com/fact-check/fact-check/nisha-dahiya-31859 new to collection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pages: 100%|█████████████████████████████████████████████████████████████████████████████| 2/2 [00:14<00:00,  7.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c-60-commandos-welcome-32020\n",
      "entered downloader\n",
      "https://thelogicalindian.com/fact-check/fact-check/c-60-commandos-welcome-32020\n",
      "c-60-commandos-welcome-32020\\file.html\n",
      "Article Already Downloaded. Loading from local.\n",
      "entered article_parser\n",
      "story has already been parsed.See  c-60-commandos-welcome-32020\\post.json\n",
      "entered media downloader\n",
      "media dictionary exists. Some media items may have been dowloaded. See  c-60-commandos-welcome-32020/media_dict.json\n",
      "entered data uploader\n",
      "https://thelogicalindian.com/fact-check/fact-check/c-60-commandos-welcome-32020 already added to database. If you want to updatex media items, use separate script\n",
      "indo-china-road-31998\n",
      "entered downloader\n",
      "https://thelogicalindian.com/fact-check/fact-check/indo-china-road-31998\n",
      "indo-china-road-31998\\file.html\n",
      "Article Already Downloaded. Loading from local.\n",
      "entered article_parser\n",
      "story has already been parsed.See  indo-china-road-31998\\post.json\n",
      "entered media downloader\n",
      "media dictionary exists. Some media items may have been dowloaded. See  indo-china-road-31998/media_dict.json\n",
      "entered data uploader\n",
      "https://thelogicalindian.com/fact-check/fact-check/indo-china-road-31998 already added to database. If you want to updatex media items, use separate script\n",
      "namaz-on-roads-31994\n",
      "entered downloader\n",
      "https://thelogicalindian.com/fact-check/fact-check/namaz-on-roads-31994\n",
      "namaz-on-roads-31994\\file.html\n",
      "Article Already Downloaded. Loading from local.\n",
      "entered article_parser\n",
      "story has already been parsed.See  namaz-on-roads-31994\\post.json\n",
      "entered media downloader\n",
      "media dictionary exists. Some media items may have been dowloaded. See  namaz-on-roads-31994/media_dict.json\n",
      "entered data uploader\n",
      "https://thelogicalindian.com/fact-check/fact-check/namaz-on-roads-31994 already added to database. If you want to updatex media items, use separate script\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print('TLI scraper initiated')\n",
    "    \n",
    "    links = crawler(crawl_url)\n",
    "    #print(links)\n",
    "    \n",
    "    for link in links:\n",
    "        sub_folder = link.split(\"/\")[-1] \n",
    "        print(sub_folder)\n",
    "        \n",
    "        if not os.path.exists(sub_folder):\n",
    "            os.mkdir(sub_folder)\n",
    "        html_text = article_downloader(link, sub_folder)\n",
    "        post = article_parser(html_text, link, domain, lang, sub_folder)\n",
    "        media_dict = media_downloader(post, sub_folder)\n",
    "        data_uploader(post, media_dict, html_text, sub_folder)\n",
    "        if (DEBUG==0):\n",
    "            shutil.rmtree  \n",
    "            \n",
    "    if (DEBUG==0):\n",
    "            os.remove(f'url_list.json')\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8313cedf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
