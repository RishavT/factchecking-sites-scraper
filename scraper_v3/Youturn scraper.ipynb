{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c17e752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time, sleep\n",
    "from datetime import date, datetime\n",
    "from dateutil.parser import parse\n",
    "from pyquery import PyQuery\n",
    "import pytz\n",
    "import json  \n",
    "from bson import json_util\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "from numpy.random import randint\n",
    "from uuid import uuid4\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from pymongo.collection import Collection\n",
    "\n",
    "from selenium import webdriver\n",
    "from lxml.html import fromstring\n",
    "import requests\n",
    "import boto3\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "MONGOURL = os.getenv('MONGOURL')\n",
    "DB_NAME = os.getenv('DB_NAME')\n",
    "COLL_NAME = os.getenv('COLL_NAME')\n",
    "BUCKET = os.environ[\"BUCKET\"]\n",
    "REGION_NAME = os.environ[\"REGION_NAME\"]\n",
    "\n",
    "CRAWL_PAGE_COUNT = 2\n",
    "DEBUG = 0\n",
    "\n",
    "crawl_url = \"https://en.youturn.in/category/factcheck\"\n",
    "domain=\"en.youturn.in\"\n",
    "lang = \"english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff44ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collection(MONGOURL, DB_NAME, COLL_NAME):\n",
    "    cli = MongoClient(MONGOURL)\n",
    "    db = cli[DB_NAME]\n",
    "    collection = db[COLL_NAME]\n",
    "\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a682cf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aws_connection():\n",
    "\n",
    "    access_id = os.environ[\"ACCESS_ID\"]\n",
    "    access_key = os.environ[\"ACCESS_KEY\"]\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        region_name=\"ap-south-1\",\n",
    "        aws_access_key_id=access_id,\n",
    "        aws_secret_access_key=access_key,\n",
    "    )\n",
    "\n",
    "    return s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa4ca099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tree(url):\n",
    "\n",
    "    html = None                        \n",
    "  \n",
    "    try:\n",
    "        html = requests.get(url)\n",
    "    except Exception as e:\n",
    "        print(f\"failed request: {e}\")\n",
    "\n",
    "    html.encoding = \"utf-8\"\n",
    "    tree = fromstring(html.content)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39c64752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_unicode(mangled):\n",
    "    return mangled.encode('latin1','ignore').decode('utf8', 'replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f380d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler(crawl_url): \n",
    "\n",
    "    print(\"entered crawler\")\n",
    "\n",
    "    file_name = 'url_list.json'\n",
    "    if os.path.exists(file_name):\n",
    "        print(\"site already been crawled. See \", file_name)\n",
    "        with open(file_name, \"r\") as f:\n",
    "            url_list = json.load(f)\n",
    "    \n",
    "    else:\n",
    "        url_list = []\n",
    "\n",
    "        coll = get_collection(MONGOURL, DB_NAME, COLL_NAME)\n",
    "\n",
    "        for page in tqdm(range(CRAWL_PAGE_COUNT), desc=\"pages: \"):\n",
    "            page_url = f\"{crawl_url}/page/{page+1}\"\n",
    "            print(page_url)\n",
    "            tree = get_tree(page_url)\n",
    "            \n",
    "            if (tree == None):\n",
    "                print(\"No HTML on Link\")\n",
    "                continue\n",
    "\n",
    "            permalinks = PyQuery(tree).find(\".post-item>a\")\n",
    "\n",
    "            for pl in permalinks:\n",
    "                link = pl.attrib['href']\n",
    "                if 'javascript:void(0)' in link:   #these links should not be scraped\n",
    "                    continue\n",
    "                if coll.count_documents({\"postURL\": link}, {}):\n",
    "                    print(link, \"exists in collection\")\n",
    "                    continue\n",
    "                else:\n",
    "                    print(link, \"new to collection\")\n",
    "                    url_list.append(link)\n",
    "            sleep(randint(5,10))            \n",
    "    \n",
    "        url_list = list(set(url_list))\n",
    "        with open(file_name, 'w') as f:\n",
    "            json.dump(url_list,f)  \n",
    "            \n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62e9b7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_downloader(url, sub_folder): \n",
    "    print(\"entered downloader\")\n",
    "    print(url)\n",
    "\n",
    "    file = \"file.html\"\n",
    "    file_name = os.path.join(sub_folder, file)\n",
    "    print(file_name)  \n",
    "\n",
    "    if os.path.exists(file_name):\n",
    "        print(\"Article Already Downloaded. Loading from local.\")\n",
    "        with open(file_name, 'rb') as f:\n",
    "            html_text = f.read()\n",
    "    else:\n",
    "        response = requests.get(url)\n",
    "        html_text = response.content\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            f.write(html_text)\n",
    "    \n",
    "    return html_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dad6dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_info(pq):\n",
    "\n",
    "    headline = pq(\"h1.post-title\").text()\n",
    "    print(headline)\n",
    "    date = pq(\"div.post-meta>span.date\").text().split(\" \")[0:3]\n",
    "    datestr = ' '.join(map(str, date))\n",
    "    print(datestr)\n",
    "    datestr = parse(datestr).astimezone(pytz.timezone('Asia/Calcutta')).strftime(\"%B %d, %Y\")\n",
    "    author_name = \"NA\"\n",
    "    author_link = \"NA\"\n",
    "    article_info = {\n",
    "        \"headline\": restore_unicode(headline),\n",
    "        \"author\": author_name,\n",
    "        \"author_link\": author_link,\n",
    "        \"date_updated\": restore_unicode(datestr),\n",
    "    }\n",
    "    print(article_info)\n",
    "    \n",
    "    return article_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fe6a81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_content(pq):\n",
    "    \n",
    "    content = {\n",
    "        \"text\": [],\n",
    "        \"fb_post\": [],\n",
    "        \"image\": [],\n",
    "        \"tweet\": [],\n",
    "        \"video\": [],\n",
    "        \"youtube\": [],\n",
    "    }\n",
    "\n",
    "    # text content\n",
    "    claim = pq('div.entry-content>div>p').text()\n",
    "    explanation = pq('div.entry-content>p').text()\n",
    "    content['text'] = \". \".join([claim, explanation])\n",
    "\n",
    "    # images\n",
    "    images = pq.find('div.featured-area-inner>figure.single-featured-image>img')\n",
    "    images += pq.find('div.entry-content>p>img')\n",
    "    images += pq.find('div.entry-content>div>p>img')\n",
    "    images += pq.find('div>img')\n",
    "    images = list(dict.fromkeys(images))\n",
    "    for i in images:\n",
    "        if not 'gif' in str(i.attrib[\"src\"]):\n",
    "            content[\"image\"].append(i.attrib[\"src\"])   \n",
    "    \n",
    "    # facebook posts\n",
    "    fb_post = pq.find('div.entry-content>div>p>a') \n",
    "    fb_post += pq.find('div.entry-content>p>a')\n",
    "    for f in fb_post:\n",
    "        if \"facebook\" in str(f.attrib[\"href\"]):\n",
    "            content[\"fb_post\"].append(f.attrib[\"href\"])\n",
    "    \n",
    "    # tweets\n",
    "    tweet = pq.find('div.entry-content>p>a')\n",
    "    tweet += pq.find('div.entry-content>div>p>a')\n",
    "    for t in tweet:\n",
    "        if \"twitter\" in str(t.attrib[\"href\"]):\n",
    "            content[\"tweet\"].append(t.attrib[\"href\"])  \n",
    "    \n",
    "    #videos\n",
    "    video = pq.find('div.entry-content>p>iframe') \n",
    "    for v in video:\n",
    "        content[\"video\"].append(v.attrib[\"src\"])  \n",
    "    \n",
    "    # youtube videos\n",
    "    youtube = pq.find('div.entry-content>div>p>a')\n",
    "    if \"yout\" in str(youtube.attr[\"href\"]):\n",
    "        content[\"youtube\"].append(youtube.attr[\"href\"])\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f57d5e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_timestamp(item_date_object):\n",
    "    if isinstance(item_date_object, (date, datetime)):\n",
    "        return item_date_object.timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bfc276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_parser(html_text, url, domain, lang, sub_folder):\n",
    "    \n",
    "    print(\"entered article_parser\")\n",
    "    file = \"post.json\"\n",
    "    file_name = os.path.join(sub_folder, file)\n",
    "    if os.path.exists(file_name):\n",
    "        print(\"story has already been parsed.See \", file_name)\n",
    "        with open(file_name, \"r\") as f:\n",
    "            post = json.load(f)\n",
    "    else:\n",
    "        print(\"entered parser\")\n",
    "        pq = PyQuery(html_text)\n",
    "        \n",
    "        # generate post_id\n",
    "        post_id = uuid4().hex\n",
    "\n",
    "        article_info = get_article_info(pq)\n",
    "\n",
    "        # uniform date format\n",
    "        now_date = date.today().strftime(\"%B %d, %Y\")\n",
    "        now_date_utc = datetime.utcnow()\n",
    "        date_updated = article_info[\"date_updated\"]\n",
    "        date_updated_utc = datetime.strptime(date_updated, \"%B %d, %Y\")\n",
    "\n",
    "        author = {\"name\": article_info[\"author\"], \"link\": article_info[\"author_link\"]}  \n",
    "\n",
    "        article_content = get_article_content(pq)\n",
    "        docs = []\n",
    "        for k, v in article_content.items():\n",
    "            if not v:  # empty list\n",
    "                continue\n",
    "            \n",
    "                        \n",
    "            if k == \"text\":  \n",
    "                doc_id = uuid4().hex\n",
    "                doc = {\n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"postID\": post_id,\n",
    "                    \"domain\": domain,\n",
    "                    \"origURL\": url, # for text content, URL is the URL of the story\n",
    "                    \"s3URL\": None,\n",
    "                    \"possibleLangs\": lang,\n",
    "                    \"mediaType\": k,\n",
    "                    \"content\": v,  # text, if media_type = text or text in image/audio/video\n",
    "                    \"nowDate\": now_date,  # date of scraping, same as date_accessed\n",
    "                    \"nowDate_UTC\": now_date_utc,\n",
    "                    \"isGoodPrior\": None,  # no of [-ve votes, +ve votes] TODO: Discuss  Discussed: Look More\n",
    "                }\n",
    "                docs.append(doc)\n",
    "            \n",
    "            else:\n",
    "                for media_url in v:\n",
    "                    doc_id = uuid4().hex\n",
    "                    print(doc_id)\n",
    "                    doc = {\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"postID\": post_id,\n",
    "                        \"domain\": domain,\n",
    "                        \"origURL\": media_url,  # for images,videos URL is the URL of the media item.\n",
    "                        \"s3URL\": None,\n",
    "                        \"possibleLangs\": lang,\n",
    "                        \"mediaType\": k,\n",
    "                        \"content\": None,  # this field is specifically to store text content.\n",
    "                        \"nowDate\": now_date,  # date of scraping, same as date_accessed\n",
    "                        \"nowDate_UTC\": now_date_utc,\n",
    "                        \"isGoodPrior\": None,  # no of [-ve votes, +ve votes] TODO: Discuss\n",
    "                    }  \n",
    "                    docs.append(doc)          \n",
    "\n",
    "        post = {\n",
    "            \"postID\": post_id,  # unique post ID\n",
    "            \"postURL\": url,  \n",
    "            \"domain\": domain,  # domain such as altnews/factly\n",
    "            \"headline\": article_info[\"headline\"],  # headline text\n",
    "            \"date_accessed\": now_date,  # date scraped\n",
    "            \"date_accessed_UTC\": now_date_utc,\n",
    "            \"date_updated\": date_updated,  # later of date published/updated\n",
    "            \"date_updated_UTC\": date_updated_utc,  # later of date published/updated\n",
    "            \"author\": author,\n",
    "            \"post_category\": None,\n",
    "            \"claims_review\": None,\n",
    "            \"docs\": docs,\n",
    "        }\n",
    "\n",
    "        print(post)\n",
    "\n",
    "\n",
    "        json_data = json.dumps(post, default=convert_timestamp)\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write(json_data)\n",
    "\n",
    "        \n",
    "    return post "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a8c5610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_images(post,sub_folder):\n",
    "\n",
    "    url = None\n",
    "    filename_dict = {}\n",
    "\n",
    "    for doc in post[\"docs\"]:\n",
    "        if (doc[\"mediaType\"] == 'image'):\n",
    "            url = doc[\"origURL\"]\n",
    "            if url is None:\n",
    "                print(\"Media url is None. Setting s3URL as error...\")\n",
    "                doc[\"s3_url\"] = \"ERR\"\n",
    "            else:\n",
    "                filename = url.split(\"/\")[-1]\n",
    "                \n",
    "                r = requests.get(url)\n",
    "                image = Image.open(BytesIO(r.content)) \n",
    "                if len(filename.split(\".\")) == 1:\n",
    "                        filename = f\"{filename}.{image.format.lower()}\"\n",
    "                if image.mode in (\"RGBA\", \"P\"): \n",
    "                    image = image.convert(\"RGB\")\n",
    "                imgfile=f'{sub_folder}/{filename}'\n",
    "                image.save(f'{sub_folder}/{filename}')\n",
    "                filename_dict.update({doc[\"doc_id\"]: filename})\n",
    "                \n",
    "    print(filename_dict)\n",
    "    return filename_dict\n",
    "\n",
    "\n",
    "def media_downloader(post, sub_folder):\n",
    "    print(\"entered media downloader\")\n",
    "    file_name = f'{sub_folder}/media_dict.json'\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        print(\"media dictionary exists. Some media items may have been dowloaded. See \",file_name)\n",
    "        with open(file_name, \"r\") as f:\n",
    "            media_dict = json.load(f)\n",
    "        \n",
    "    else:    \n",
    "        media_dict = get_all_images(post,sub_folder)\n",
    "        json_data = json.dumps(media_dict, default=convert_timestamp)\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write(json_data)\n",
    "\n",
    "    return media_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48151622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_uploader(post, media_dict, html_text, sub_folder):\n",
    "\n",
    "    print(\"entered data uploader\")\n",
    "\n",
    "    coll = get_collection(MONGOURL, DB_NAME, COLL_NAME)\n",
    "     \n",
    "    if coll.count_documents({\"postURL\": post[\"postURL\"]}, {}):\n",
    "        print(post[\"postURL\"], \"already added to database. If you want to updatex media items, use separate script\")\n",
    "        return 1\n",
    "                \n",
    "\n",
    "    s3 = aws_connection()\n",
    "\n",
    "    for doc in post[\"docs\"]:\n",
    "            if (doc[\"s3URL\"] != None):\n",
    "                print(\"Skipping upload. Doc has an existing s3_url:\",doc[\"s3URL\"])\n",
    "                continue\n",
    "            filename = media_dict.get(doc[\"doc_id\"])\n",
    "            if (filename != None):\n",
    "                s3_filename = str(uuid4())\n",
    "                res = s3.upload_file(\n",
    "                            f'{sub_folder}/{filename}',\n",
    "                            BUCKET,\n",
    "                            s3_filename,\n",
    "                            ExtraArgs={\"ContentType\": \"unk_content_type\"},\n",
    "                        )\n",
    "                s3_url = f\"https://{BUCKET}.s3.{REGION_NAME}.amazonaws.com/{s3_filename}\" \n",
    "                doc[\"s3URL\"] = s3_url\n",
    "                \n",
    "            \n",
    "\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "  \n",
    "    coll.insert_one(post)\n",
    "\n",
    "    s3_html_name = post[\"postURL\"]\n",
    "    file = \"file.html\"\n",
    "    res = s3.upload_file( os.path.join(sub_folder, file),\n",
    "                          BUCKET,\n",
    "                          s3_html_name,\n",
    "                          ExtraArgs={\"ContentType\": \"unk_content_type\"},\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b20832e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print('Youturn scraper initiated')\n",
    "    \n",
    "    links = crawler(crawl_url)\n",
    "    print(links)\n",
    "\n",
    "    \n",
    "    for link in links:\n",
    "        sub_folder = link.split(\"/\")[-1].split(\".\")[0] \n",
    "        print(sub_folder)\n",
    "        \n",
    "        if not os.path.exists(sub_folder):\n",
    "            os.mkdir(sub_folder)\n",
    "        html_text = article_downloader(link, sub_folder)\n",
    "        post = article_parser(html_text, link, domain, lang, sub_folder)\n",
    "        media_dict = media_downloader(post, sub_folder)\n",
    "        data_uploader(post, media_dict, html_text, sub_folder)\n",
    "        if (DEBUG==0):\n",
    "            shutil.rmtree  \n",
    "            \n",
    "    if (DEBUG==0):\n",
    "            os.remove(f'url_list.json')\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d7eb3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
